<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Detection and Emotion Recognition Project</title>
    <style>
        body {
            font-family: sans-serif;
            margin: 20px;
        }
        h1, h2, h3 {
            color: #333;
        }
        .project {
            border: 1px solid #ccc;
            padding: 20px;
            margin-bottom: 20px;
            border-radius: 5px;
        }
        .project-title {
            font-size: 1.2em;
            margin-bottom: 10px;
        }
        .project-description {
            margin-bottom: 10px;
        }
        .technologies {
            font-style: italic;
            margin-bottom: 10px;
        }
        .how-it-works {
            margin-bottom: 10px;
        }
        .implementation {
            margin-bottom: 10px;
        }
        .challenges {
            margin-bottom: 10px;
        }
        .tools {
            margin-bottom: 10px;
        }
        .code-example {
            background-color: #f0f0f0;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
            font-family: monospace;
        }
        #video {
            width: 640px;
            height: 480px;
            border: 1px solid #ccc;
        }
        #canvas {
            position: absolute;
            top: 0;
            left: 0;
        }
    </style>
</head>
<body>

    <h1>My Projects</h1>

    <div class="project">
        <h2 class="project-title">Face Detection and Emotion Recognition</h2>
        <div class="project-description">
            This project uses web technologies (HTML, CSS, and JavaScript) and the Face API library to detect faces in a video stream and recognize emotions based on facial expressions.  It's a demonstration of real-time computer vision in the browser.
        </div>
        <div class="technologies">
            Technologies: HTML, CSS, JavaScript, Face API
        </div>

        <div class="how-it-works">
            <h3>How It Works:</h3>
            <p>
                The project accesses the user's webcam using the `navigator.mediaDevices.getUserMedia()` API. The Face API library is loaded and used to detect faces in the video frames.  The Face API returns facial landmarks and expression probabilities.  These are then used to determine the dominant emotion. The results (bounding boxes, emotion labels) are drawn on a canvas overlaid on the video.
            </p>
        </div>

        <div class="implementation">
            <h3>Implementation Details:</h3>
            <p>
                The HTML includes a `<video>` element for the webcam feed and a `<canvas>` for drawing. The JavaScript code handles:
                <ol>
                    <li>Accessing the webcam.</li>
                    <li>Loading the Face API models.</li>
                    <li>Processing each video frame:</li>
                        <ul>
                            <li>Detecting faces using `faceapi.detectAllFaces()`.</li>
                            <li>Getting face landmarks and expressions using `.withFaceLandmarks().withFaceExpressions()`.</li>
                            <li>Drawing bounding boxes and emotion labels on the canvas.</li>
                        </ul>
                </ol>
            </p>
        </div>

        <div class="challenges">
            <h3>Challenges:</h3>
            <p>
                Real-time performance is a key challenge.  Face detection and emotion recognition are computationally intensive.  Accurate emotion recognition is difficult due to variations in lighting, pose, and individual expressions. The Face API helps with this, but it's not perfect.
            </p>
        </div>

        <div class="tools">
            <h3>Tools:</h3>
            <p>
                VS Code (or any code editor), a web browser (Chrome recommended for best compatibility), and the Face API library.
            </p>
        </div>

        <div class="code-example">
            <h3>Code Example (Simplified):</h3>
            <pre>
                <code>
// In your JavaScript file:
async function loadModels() {
    await faceapi.nets.ssdMobilenetv1.loadFromUri('/models'); // Load face detection model
    await faceapi.nets.faceLandmark68Net.loadFromUri('/models'); // Load landmark model
    await faceapi.nets.faceExpressionNet.loadFromUri('/models'); // Load expression model
}

async function detectAndDraw(video, canvas) {
    const detections = await faceapi.detectAllFaces(video).withFaceLandmarks().withFaceExpressions();
    const resizedDetections = faceapi.resizeDetections(detections, displaySize); // Resize if needed
    faceapi.draw.drawDetections(canvas, resizedDetections);
    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
    faceapi.draw.drawFaceExpressions(canvas, resizedDetections);
}

// ... (Rest of the code for webcam access, video processing, etc.)
                </code>
            </pre>
        </div>

        <div>
            <video id="video" autoplay muted></video>
            <canvas id="canvas"></canvas>
        </div>

        <script src="face-api.min.js"></script> <script>
        // JavaScript code for webcam access, face detection, and emotion recognition.
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        let displaySize;

        Promise.all([
            faceapi.nets.ssdMobilenetv1.loadFromUri('/models'),
            faceapi.nets.faceLandmark68Net.loadFromUri('/models'),
            faceapi.nets.faceExpressionNet.loadFromUri('/models'),
        ]).then(startVideo)

        function startVideo() {
            navigator.mediaDevices.getUserMedia({ video: true })
                .then(stream => {
                    video.srcObject = stream;
                })
                .catch(err => console.error("Error accessing webcam:", err));
        }

        video.addEventListener('play', () => {
            displaySize = { width: video.width, height: video.height };
            faceapi.matchDimensions(canvas, displaySize);
            setInterval(async () => {
                const detections = await faceapi.detectAllFaces(video, new faceapi.SsdMobilenetv1()).withFaceLandmarks().withFaceExpressions();
                const resizedDetections = faceapi.resizeDetections(detections, displaySize)
                canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)
                faceapi.draw.drawDetections(canvas, resizedDetections)
                faceapi.draw.drawFaceLandmarks(canvas, resizedDetections)
                faceapi.draw.drawFaceExpressions(canvas, resizedDetections)
            }, 100)
        })

    </script>

    </div>

</body>
</html>